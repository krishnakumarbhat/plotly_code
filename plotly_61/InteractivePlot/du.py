# # 1. Offloading Memory to Disk : When figures are generated by individual processes in a multiprocessing pool, they are immediately serialized and written to JSON files on disk. This allows the memory occupied by these figures to be released from the generating process once the data is persisted. Without this, each process might hold a copy of the generated figures in memory until all processing is complete, leading to high memory consumption.
# # 2. Reduced Inter-Process Communication Overhead : While not directly memory reduction, passing large Python objects (like Plotly figures) between processes can be memory-intensive due to serialization/deserialization overhead. By writing to disk, processes only need to pass file paths,  which are lightweight.


# # 3.Optimize multiprocessing : Fine-tune the number of worker processes based on available memory


# # Can we split allsensorhdfparser.py and dataprep.py into three separate Python files to reduce the code length in both allsensor and dataprep.py?so that all three files are balanced, as those 2  are tightly coupled(remove tight coplying here "DataPrep(input_data....)) .

# # Additionally, regarding the use of DataPrep(input_data..) in a nested for loop:
# # Currently, calling it outside the loop processes all streams at once, which increases memory usage. I want to process only a specific number of streams (e.g., 2 or 3) as needed. Is it possible to modify the code to allow selecting only the required number of streams for processing?






# import h5py
# import os
# import logging
# import time
# from typing import Dict, List
# from InteractivePlot.b_persistence_layer import hdf_parser
# from InteractivePlot.d_business_layer.data_prep import DataPrep
# from InteractivePlot.b_persistence_layer.Persensor_hdf_parser import PersensorHdfParser
# from InteractivePlot.b_persistence_layer.prerun_hdf_parser import PreRun
# from InteractivePlot.b_persistence_layer.data_processor import DataProcessor
# from InteractivePlot.c_data_storage.data_model_storage import DataModelStorage
# from InteractivePlot.c_data_storage.regex_storage import Gen7V1_v2_streams
# import asyncio


# class AllsensorHdfParser(PersensorHdfParser):
#     """Parser for HDF5 files based on an address map and customer type."""

#     def __init__(self, address_map: Dict[str, str], output_dir=None):
#         """
#         Initialize the AllsensorHdfParser.

#         Args:
#             address_map: Dictionary mapping input file paths to output file paths
#             output_dir: Directory to save HTML reports
#         """
#         super().__init__(address_map, output_dir)
#         self.start_time_parsing = time.time()
        
#         # Instance variables to replace globals
#         self.base_name = None
#         self.base_name_out = None
#         self.hdf_file_in = None
#         self.hdf_file_out = None
        
#         # Data processor for handling DataPrep operations
#         self.data_processor = DataProcessor(output_dir)

#     def parse(self) -> List[DataPrep]:
#         """
#         Parse input and output HDF5 files based on the address map.
#         Uses sequential processing to avoid serialization issues.

#         Returns:
#             List[DataPrep]: List of DataPrep objects for each sensor and stream
#         """

#         for input_file, output_file in self.address_map.items():
#             # Set instance variables instead of globals
#             self.base_name = os.path.basename(input_file).split(".")[0]
#             self.base_name_out = os.path.basename(output_file).split(".")[0]
#             logging.info(f"\nProcessing input file: {os.path.basename(input_file)}")
#             logging.info(f"Processing output file: {os.path.basename(output_file)} \n")
            
#             # Parse file metadata
#             parsed_data = self._parse_file_metadata(input_file, output_file)
#             if not parsed_data:
#                 continue
                
#             missing_data, sensor_list, streams = parsed_data
            
#             # Open HDF5 files as instance variables
#             if not self._open_hdf_files(input_file, output_file):
#                 continue
                
#             try:
#                 # Process all sensor-stream combinations
#                 self._process_sensor_stream_combinations(
#                     sensor_list, streams, missing_data
#                 )
#             finally:
#                 # Ensure files are properly closed
#                 self._close_hdf_files()

#             print("\nCompleted processing all sensor/stream combinations")
            
#         # Return all processed DataPrep objects
#         return self.data_processor.get_processed_data()
    
#     def _parse_file_metadata(self, input_file: str, output_file: str):
#         """
#         Parse file metadata using PreRun.
        
#         Args:
#             input_file: Path to input HDF5 file
#             output_file: Path to output HDF5 file
            
#         Returns:
#             Tuple of (missing_data, sensor_list, streams) or None if error
#         """
#         try:
#             prerun_result = PreRun(input_file, output_file)
#             missing_data = prerun_result.missing_data
#             sensor_list = prerun_result.sensor_list
#             streams = prerun_result.streams

#             if missing_data:
#                 logging.info(
#                     f"Warning: Missing data if in input not in output or viceversa : {missing_data}"
#                 )

#             logging.info(f"Found {len(sensor_list)} sensors: {', '.join(sensor_list)}")
#             logging.info(f"Found {len(streams)} streams: {', '.join(streams)}")
            
#             return missing_data, sensor_list, streams
            
#         except Exception as e:
#             logging.error(f"Error parsing file metadata: {str(e)}")
#             return None
    
#     def _open_hdf_files(self, input_file: str, output_file: str) -> bool:
#         """
#         Open HDF5 files and store as instance variables.
        
#         Args:
#             input_file: Path to input HDF5 file
#             output_file: Path to output HDF5 file
            
#         Returns:
#             bool: True if both files opened successfully, False otherwise
#         """
#         try:
#             self.hdf_file_in = h5py.File(input_file, "r")
#         except Exception as e:
#             logging.error(f"Error opening input file {input_file}: {str(e)}")
#             return False

#         try:
#             self.hdf_file_out = h5py.File(output_file, "r")
#         except Exception as e:
#             logging.error(f"Error opening output file {output_file}: {str(e)}")
#             if self.hdf_file_in:
#                 self.hdf_file_in.close()
#             return False
            
#         return True
    
#     def _close_hdf_files(self):
#         """
#         Close HDF5 files if they are open.
#         """
#         if self.hdf_file_in:
#             try:
#                 self.hdf_file_in.close()
#             except Exception as e:
#                 logging.error(f"Error closing input file: {str(e)}")
#             finally:
#                 self.hdf_file_in = None
                
#         if self.hdf_file_out:
#             try:
#                 self.hdf_file_out.close()
#             except Exception as e:
#                 logging.error(f"Error closing output file: {str(e)}")
#             finally:
#                 self.hdf_file_out = None
    
#     def _process_sensor_stream_combinations(self, sensor_list: List[str], streams: List[str], missing_data):
#         """
#         Process all sensor-stream combinations.
        
#         Args:
#             sensor_list: List of sensor identifiers
#             streams: List of stream identifiers
#             missing_data: Missing data information from PreRun
#         """
#         total_combinations = len(sensor_list) * len(streams)
#         processed = 0
        
#         for sensor in sensor_list:
#             html_name = f"{self.base_name}_{sensor}.html"
            
#             for stream in streams:
#                 if stream in Gen7V1_v2_streams:
#                     processed += 1
#                     progress = (processed / total_combinations) * 100
#                     print(f"\rProcessing...... [{progress:.1f}%]", end="")

#                     # Parse data for this sensor-stream combination
#                     input_data, output_data = self._parse_sensor_stream_data(sensor, stream)
                    
#                     # Process through DataProcessor (decoupled from parsing)
#                     if input_data or output_data:
#                         self.data_processor.process_sensor_stream_data(
#                             input_data,
#                             output_data,
#                             html_name,
#                             sensor,
#                             stream,
#                             self.base_name,
#                             self.base_name_out,
#                             generate_html=True
#                         )

#             logging.info(
#                 f"Generated HTML report for sensor /{sensor}/ with /{len(streams)}/ streams"
#             )
    
#     def _parse_sensor_stream_data(self, sensor: str, stream: str):
#         """
#         Parse data for a specific sensor-stream combination.
        
#         Args:
#             sensor: Sensor identifier
#             stream: Stream identifier
            
#         Returns:
#             Tuple of (input_data, output_data) DataModelStorage objects
#         """
#         # Create storage instances for this combination
#         input_data = DataModelStorage()
#         output_data = DataModelStorage()

#         input_data.init_parent(stream)
#         output_data.init_parent(stream)

#         # Process input file
#         sensor_stream_path = f"{sensor}/{stream}"
#         input_data = self._parse_hdf_data(self.hdf_file_in, sensor_stream_path, input_data, sensor, stream, "input")
        
#         # Process output file
#         output_data = self._parse_hdf_data(self.hdf_file_out, sensor_stream_path, output_data, sensor, stream, "output")
        
#         return input_data, output_data
    
#     def _parse_hdf_data(self, hdf_file, sensor_stream_path: str, data_storage: DataModelStorage, sensor: str, stream: str, file_type: str):
#         """
#         Parse HDF5 data for a specific sensor-stream path.
        
#         Args:
#             hdf_file: Open HDF5 file handle
#             sensor_stream_path: Path to sensor/stream data in HDF5 file
#             data_storage: DataModelStorage object to populate
#             sensor: Sensor identifier
#             stream: Stream identifier
#             file_type: "input" or "output" for logging purposes
            
#         Returns:
#             DataModelStorage: Populated data storage object
#         """
#         if sensor_stream_path in hdf_file:
#             data_group = hdf_file[sensor_stream_path]

#             # Find header using next() with generator expression
#             header_variants = [
#                 "Stream_Hdr",
#                 "stream_hdr",
#                 "StreamHdr",
#                 "STREAM_HDR",
#                 "streamheader",
#                 "stream_header",
#                 "HEADER_STREAM",
#             ]
#             header_path = next(
#                 (
#                     variant
#                     for variant in header_variants
#                     if variant in data_group
#                 ),
#                 None,
#             )

#             if header_path:
#                 scan_index = data_group[f"{header_path}/scan_index"][()]
#                 data_storage.initialize(scan_index, sensor, stream)
#                 start_time = time.time()
#                 data_storage = hdf_parser.HDF5Parser.parse(
#                     data_group, data_storage, scan_index, header_variants
#                 )
#                 end_time = time.time()
#                 logging.debug(
#                     f"Time taken by {file_type} parsing {end_time - start_time} in {stream}"
#                 )
        
#         return data_storage




























# from typing import Dict, List
# from InteractivePlot.d_business_layer.data_prep import DataPrep
# from InteractivePlot.b_persistence_layer.Persensor_hdf_parser import PersensorHdfParser
# from InteractivePlot.c_data_storage.data_model_storage import DataModelStorage


# class AllsensorHdfParser(PersensorHdfParser):
#     """Parser for HDF5 files based on an address map and customer type."""

#     def __init__(self, address_map: Dict[str, str], output_dir=None):
#         """
#         Initialize the AllsensorHdfParser.

#         Args:
#             address_map: Dictionary mapping input file paths to output file paths
#             output_dir: Directory to save HTML reports
#         """
#         super().__init__(address_map, output_dir)
#         self.start_time_parsing = time.time()
        
#         # Instance variables to replace globals
#         self.base_name = None
#         self.base_name_out = None
#         self.hdf_file_in = None
#         self.hdf_file_out = None
        
#         # Data processor for handling DataPrep operations
#         self.data_processor = DataProcessor(output_dir)

#     def parse(self) -> List[DataPrep]:
#         """
#         Parse input and output HDF5 files based on the address map.
#         Uses sequential processing to avoid serialization issues.

#         Returns:
#             List[DataPrep]: List of DataPrep objects for each sensor and stream
#         """

#         for input_file, output_file in self.address_map.items():
#             # Set instance variables instead of globals
#             self.base_name = os.path.basename(input_file).split(".")[0]
#             self.base_name_out = os.path.basename(output_file).split(".")[0]
#             logging.info(f"\nProcessing input file: {os.path.basename(input_file)}")
#             logging.info(f"Processing output file: {os.path.basename(output_file)} \n")
            
#             # Parse file metadata
#             parsed_data = self._parse_file_metadata(input_file, output_file)
#             if not parsed_data:
#                 continue
                
#             missing_data, sensor_list, streams = parsed_data
            
#             # Open HDF5 files as instance variables
#             self.hdf_file_in = None
#             self.hdf_file_out = None
            
#             try:
#                 self.hdf_file_in = h5py.File(input_file, "r")
#                 self.hdf_file_out = h5py.File(output_file, "r")
#             except Exception as e:
#                 logging.error(f"Error opening HDF files: {str(e)}")
#                 self._close_hdf_files()
#                 continue
                
#             try:
#                 # Process all sensor-stream combinations
#                 self._process_sensor_stream_combinations_sync(
#                     sensor_list, streams, missing_data
#                 )
#             finally:
#                 # Ensure files are properly closed
#                 self._close_hdf_files()

#             print("\nCompleted processing all sensor/stream combinations")
            
#         # Return all processed DataPrep objects
#         return self.data_processor.get_processed_data()
    
#     def _close_hdf_files(self):
#         """
#         Close HDF5 files if they are open.
#         """
#         if self.hdf_file_in:
#             try:
#                 self.hdf_file_in.close()
#             except Exception as e:
#                 logging.error(f"Error closing input file: {str(e)}")
#             finally:
#                 self.hdf_file_in = None
                
#         if self.hdf_file_out:
#             try:
#                 self.hdf_file_out.close()
#             except Exception as e:
#                 logging.error(f"Error closing output file: {str(e)}")
#             finally:
#                 self.hdf_file_out = None
    
#     def _process_sensor_stream_combinations_sync(self, sensor_list: List[str], streams: List[str], missing_data):
#         """
#         Process all sensor-stream combinations synchronously.
        
#         Args:
#             sensor_list: List of sensor identifiers
#             streams: List of stream identifiers
#             missing_data: Missing data information from PreRun
#         """
#         total_combinations = len(sensor_list) * len(streams)
#         processed = 0
        
#         for sensor in sensor_list:
#             html_name = f"{self.base_name}_{sensor}.html"
            
#             for stream in streams:
#                 if stream in Gen7V1_v2_streams:
#                     processed += 1
#                     progress = (processed / total_combinations) * 100
#                     print(f"\rProcessing...... [{progress:.1f}%]", end="")

#                     # Parse data for this sensor-stream combination
#                     input_data, output_data = self._parse_sensor_stream_data_sync(sensor, stream)
                    
#                     # Process through DataProcessor (decoupled from parsing)
#                     if input_data or output_data:
#                         self.data_processor.process_sensor_stream_data(
#                             input_data,
#                             output_data,
#                             html_name,
#                             sensor,
#                             stream,
#                             self.base_name,
#                             self.base_name_out,
#                             generate_html=True
#                         )

#             logging.info(
#                 f"Generated HTML report for sensor /{sensor}/ with /{len(streams)}/ streams"
#             )
    
#     def _parse_sensor_stream_data_sync(self, sensor: str, stream: str):
#         """
#         Parse data for a specific sensor-stream combination synchronously.
        
#         Args:
#             sensor: Sensor identifier
#             stream: Stream identifier
            
#         Returns:
#             Tuple of (input_data, output_data) DataModelStorage objects
#         """
#         # Create storage instances for this combination
#         input_data = DataModelStorage()
#         output_data = DataModelStorage()

#         input_data.init_parent(stream)
#         output_data.init_parent(stream)

#         # Process input file
#         sensor_stream_path = f"{sensor}/{stream}"
#         input_data = self._parse_hdf_data(self.hdf_file_in, sensor_stream_path, input_data, sensor, stream, "input")
        
#         # Process output file
#         output_data = self._parse_hdf_data(self.hdf_file_out, sensor_stream_path, output_data, sensor, stream, "output")
        
#         return input_data, output_data
        
#     async def async_parse_files(self) -> List[DataPrep]:
#         """
#         Parse input and output HDF5 files based on the address map asynchronously.
#         Uses concurrent processing for better performance.

#         Returns:
#             List[DataPrep]: List of DataPrep objects for each sensor and stream
#         """
#         # Create tasks for each file pair
#         tasks = []
        
#         for input_file, output_file in self.address_map.items():
#             # Create a task for each file pair
#             task = asyncio.create_task(
#                 self._process_file_pair_async(input_file, output_file)
#             )
#             tasks.append(task)
            
#         # Wait for all tasks to complete
#         await asyncio.gather(*tasks)
            
#         # Return all processed DataPrep objects
#         return self.data_processor.get_processed_data()
    
#     async def _process_file_pair_async(self, input_file: str, output_file: str):
#         """
#         Process a single pair of input/output files asynchronously.
        
#         Args:
#             input_file: Path to input HDF5 file
#             output_file: Path to output HDF5 file
#         """
#         # Set instance variables for this file pair
#         base_name = os.path.basename(input_file).split(".")[0]
#         base_name_out = os.path.basename(output_file).split(".")[0]
        
#         logging.info(f"\nProcessing input file: {os.path.basename(input_file)}")
#         logging.info(f"Processing output file: {os.path.basename(output_file)} \n")
        
#         # Parse file metadata
#         loop = asyncio.get_event_loop()
#         parsed_data = await loop.run_in_executor(
#             None, self._parse_file_metadata, input_file, output_file
#         )
        
#         if not parsed_data:
#             return
            
#         missing_data, sensor_list, streams = parsed_data
        
#         # Open HDF5 files
#         hdf_file_in = None
#         hdf_file_out = None
        
#         try:
#             # Open files in executor to avoid blocking
#             hdf_file_in, hdf_file_out = await loop.run_in_executor(
#                 None,
#                 lambda: self._open_files_for_async(input_file, output_file)
#             )
            
#             if not hdf_file_in or not hdf_file_out:
#                 return
                
#             # Process sensor-stream combinations concurrently
#             await self._process_combinations_async(
#                 hdf_file_in, hdf_file_out, sensor_list, streams, 
#                 missing_data, base_name, base_name_out
#             )
            
#         finally:
#             # Close files in executor
#             if hdf_file_in or hdf_file_out:
#                 await loop.run_in_executor(
#                     None,
#                     lambda: self._close_files_for_async(hdf_file_in, hdf_file_out)
#                 )
                
#         logging.info("Completed processing all sensor/stream combinations for this file pair")
    
#     def _open_files_for_async(self, input_file: str, output_file: str):
#         """
#         Open HDF5 files for async processing.
        
#         Args:
#             input_file: Path to input HDF5 file
#             output_file: Path to output HDF5 file
            
#         Returns:
#             Tuple of (input_file_handle, output_file_handle) or (None, None) if error
#         """
#         hdf_file_in = None
#         hdf_file_out = None
        
#         try:
#             hdf_file_in = h5py.File(input_file, "r")
#         except Exception as e:
#             logging.error(f"Error opening input file {input_file}: {str(e)}")
#             return None, None

#         try:
#             hdf_file_out = h5py.File(output_file, "r")
#         except Exception as e:
#             logging.error(f"Error opening output file {output_file}: {str(e)}")
#             if hdf_file_in:
#                 hdf_file_in.close()
#             return None, None
            
#         return hdf_file_in, hdf_file_out
    
#     def _close_files_for_async(self, hdf_file_in, hdf_file_out):
#         """
#         Close HDF5 files for async processing.
        
#         Args:
#             hdf_file_in: Input file handle
#             hdf_file_out: Output file handle
#         """
#         if hdf_file_in:
#             try:
#                 hdf_file_in.close()
#             except Exception as e:
#                 logging.error(f"Error closing input file: {str(e)}")
                
#         if hdf_file_out:
#             try:
#                 hdf_file_out.close()
#             except Exception as e:
#                 logging.error(f"Error closing output file: {str(e)}")
    
#     async def _process_combinations_async(self, hdf_file_in, hdf_file_out, sensor_list, streams, 
#                                         missing_data, base_name, base_name_out):
#         """
#         Process sensor-stream combinations asynchronously.
        
#         Args:
#             hdf_file_in: Input file handle
#             hdf_file_out: Output file handle
#             sensor_list: List of sensor identifiers
#             streams: List of stream identifiers
#             missing_data: Missing data information
#             base_name: Base name for input file
#             base_name_out: Base name for output file
#         """
#         total_combinations = len(sensor_list) * len(streams)
#         processed = 0
        
#         # Create tasks for each sensor-stream combination
#         tasks = []
        
#         for sensor in sensor_list:
#             html_name = f"{base_name}_{sensor}.html"
            
#             for stream in streams:
#                 if stream in Gen7V1_v2_streams:
#                     processed += 1
#                     progress = (processed / total_combinations) * 100
#                     print(f"\rProcessing...... [{progress:.1f}%]", end="")

#                     # Create task for this combination
#                     task = asyncio.create_task(
#                         self._process_combination_async(
#                             hdf_file_in, hdf_file_out, sensor, stream, 
#                             html_name, base_name, base_name_out
#                         )
#                     )
#                     tasks.append(task)
            
#             logging.info(
#                 f"Generated HTML report for sensor /{sensor}/ with /{len(streams)}/ streams"
#             )
        
#         # Wait for all tasks to complete
#         await asyncio.gather(*tasks)
    
#     async def _process_combination_async(self, hdf_file_in, hdf_file_out, sensor, stream, 
#                                        html_name, base_name, base_name_out):
#         """
#         Process a single sensor-stream combination asynchronously.
        
#         Args:
#             hdf_file_in: Input file handle
#             hdf_file_out: Output file handle
#             sensor: Sensor identifier
#             stream: Stream identifier
#             html_name: HTML file name
#             base_name: Base name for input file
#             base_name_out: Base name for output file
#         """
#         loop = asyncio.get_event_loop()
        
#         # Parse data for this combination in executor
#         input_data, output_data = await loop.run_in_executor(
#             None,
#             lambda: self._parse_combination_data(hdf_file_in, hdf_file_out, sensor, stream)
#         )
        
#         # Process through DataProcessor if data exists
#         if input_data or output_data:
#             await loop.run_in_executor(
#                 None,
#                 lambda: self.data_processor.process_sensor_stream_data(
#                     input_data,
#                     output_data,
#                     html_name,
#                     sensor,
#                     stream,
#                     base_name,
#                     base_name_out,
#                     generate_html=True
#                 )
#             )
    
#     def _parse_combination_data(self, hdf_file_in, hdf_file_out, sensor, stream):
#         """
#         Parse data for a specific sensor-stream combination.
        
#         Args:
#             hdf_file_in: Input file handle
#             hdf_file_out: Output file handle
#             sensor: Sensor identifier
#             stream: Stream identifier
            
#         Returns:
#             Tuple of (input_data, output_data) DataModelStorage objects
#         """
#         # Create storage instances for this combination
#         input_data = DataModelStorage()
#         output_data = DataModelStorage()

#         input_data.init_parent(stream)
#         output_data.init_parent(stream)

#         # Process input file
#         sensor_stream_path = f"{sensor}/{stream}"
#         input_data = self._parse_hdf_data(hdf_file_in, sensor_stream_path, input_data, sensor, stream, "input")
        
#         # Process output file
#         output_data = self._parse_hdf_data(hdf_file_out, sensor_stream_path, output_data, sensor, stream, "output")
        
#         return input_data, output_data
    
#     def _parse_file_metadata(self, input_file: str, output_file: str):
#         """
#         Parse file metadata using PreRun.
        
#         Args:
#             input_file: Path to input HDF5 file
#             output_file: Path to output HDF5 file
            
#         Returns:
#             Tuple of (missing_data, sensor_list, streams) or None if error
#         """
#         try:
#             prerun_result = PreRun(input_file, output_file)
#             missing_data = prerun_result.missing_data
#             sensor_list = prerun_result.sensor_list
#             streams = prerun_result.streams

#             if missing_data:
#                 logging.info(
#                     f"Warning: Missing data if in input not in output or viceversa : {missing_data}"
#                 )

#             logging.info(f"Found {len(sensor_list)} sensors: {', '.join(sensor_list)}")
#             logging.info(f"Found {len(streams)} streams: {', '.join(streams)}")
            
#             return missing_data, sensor_list, streams
            
#         except Exception as e:
#             logging.error(f"Error parsing file metadata: {str(e)}")
#             return None
    
#     def _parse_hdf_data(self, hdf_file, sensor_stream_path: str, data_storage: DataModelStorage, sensor: str, stream: str, file_type: str):
#         """
#         Parse HDF5 data for a specific sensor-stream path.
        
#         Args:
#             hdf_file: Open HDF5 file handle
#             sensor_stream_path: Path to sensor/stream data in HDF5 file
#             data_storage: DataModelStorage object to populate
#             sensor: Sensor identifier
#             stream: Stream identifier
#             file_type: "input" or "output" for logging purposes
            
#         Returns:
#             DataModelStorage: Populated data storage object
#         """
#         if sensor_stream_path in hdf_file:
#             data_group = hdf_file[sensor_stream_path]

#             # Find header using next() with generator expression
#             header_variants = [
#                 "Stream_Hdr",
#                 "stream_hdr",
#                 "StreamHdr",
#                 "STREAM_HDR",
#                 "streamheader",
#                 "stream_header",
#                 "HEADER_STREAM",
#             ]
#             header_path = next(
#                 (
#                     variant
#                     for variant in header_variants
#                     if variant in data_group
#                 ),
#                 None,
#             )

#             if header_path:
#                 scan_index = data_group[f"{header_path}/scan_index"][()]
#                 data_storage.initialize(scan_index, sensor, stream)
#                 start_time = time.time()
#                 data_storage = hdf_parser.HDF5Parser.parse(
#                     data_group, data_storage, scan_index, header_variants
#                 )
#                 end_time = time.time()
#                 logging.debug(
#                     f"Time taken by {file_type} parsing {end_time - start_time} in {stream}"
#                 )
        
#         return data_storage



